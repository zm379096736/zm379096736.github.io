title: >-
  论文阅读 Paragraph-level Neural Question Generation with Maxout Pointer and Gated
  Self-attention Networks
author: Morphling
tags:
  - 问题生成
categories:
  - 论文阅读
date: 2020-03-17 22:14:00
---
**EMNLP 2018**
### Abstract 
这篇主要针对的任务是答案为文章中片段的问题生成，问题生成可以用在问答系统和语音助手以及教育等领域上。现存的模型大多数都是以一或两句作为输入，长文本仍然在 seq2seq 模型表现不是很理想（以整段作为输入）。这篇文章提出了 maxout pointer mechansim with gated self-attention encoder 去处理长文本的问题。BLUE-4 分数达到了 **16.3**。

---
### Introduction
QG 的应用场景：QnA and conversation systems, education (H&S， 2010)
QG 的资料集与阅读理解的资料集很相似，比如 SQuAD 和 MS MARCO。
这篇文章处理的 QG 类型是 answer-aware，输入为文章和答案，输出是以答案为目标的问题。答案是在文章中的一段。
(Du Et al., 2017) 指出大概在 SQuAD 20% 的问题需要整个段落的信息来回答，但是整个段落包含了很多无关的信息，所以怎么利用好在段落中有关的信息是一个挑战。在此之前的 para-level 模型表现都不是很理想。
这篇文章提出了一个 seq2seq attenion model， 包含了 maxout pointer mechansim 和 gated self-attention 来处理 para-level 的输入。

---
### Model
![upload successful](/images/pasted-1.png)
#### Problem definition
P: 文章(可为句子或者段落)
A: 答案
任务是找到 $\overline{Q}$:

\begin{equation}
	\overline{Q} = \mathop{\arg\max}_{Q} Prob(Q|P, A)
\end{equation}

Q可能是来自于文章或者字典
#### Passage and Answer Encoding
##### encoding process

\begin{equation}
	u_{t} = RNN^{E}(u_{t-1}, [e_{t}, m_{t}])
\end{equation}

$u_{t}$ 表示 RNN 在时间 t 的隐藏状态，$e_{t}$ 表示 $x_{t}$ 在 文章 P 的 word embedding，$m_{t}$ 表示该词是否在答案中，$[a, b]$ 为 concatenation a 和 b。这篇文章把该方法称为 answer tagging，目的是为了可以生成与答案更相关的问题。
如果 $RNN^{e}$ 是双向的话，$ U = {[\overrightarrow{u_{t}},\overleftarrow{u_{t}}]} $

##### gated self-attention
**该部分内容参考 [xinze](https://www.cnblogs.com/LuoboLiam/p/11688786.html)**
gated self-attention 设计用来整合整个文章的信息还有嵌入文章内部的依赖关系，在每一时间步中优化文章和答案的嵌入表示。
分为两步，一、$a_{t}^{s}$ 是段落中所有 encode 的单词对当前 $t$ 时刻所对应单词的之间的依赖关系的系数，$U$ 表示从1到最后时刻所有的 hidden state 组成的矩阵，即表示 passage-answer；${s_t}$ 表示段落中所有 encode 的单词对当前$t$时刻所对应单词的之间的依赖关系，也是 self matching 的表示。类似于self-attention，主要目的是计算文章中不同单词对于生成问题的重要性（相关性）

\begin{equation}
	a_{t}^{s} = softmax(U^{T}W^{s}u_{t})
\end{equation}

\begin{equation}
	s_{t} = U·a_{t}^{s}
\end{equation}


二、${f_t}$ 表示新的包含 self matching 信息的 passage-answer 表示，${g_t}$ 表示一个可学习的门控单元，最后，${\hat u_t}$ 表示新的 passage-answer 表示，用来喂给decoder。gated attention 可以专注于 answer 与当前段落之间的关系。

\begin{equation}
	{f_t} = \tanh ({W^f}[{u_t},{s_t}])
\end{equation}

\begin{equation}
	{g_t} = sigmoid({W^g}[{u_t},{s_t}])
\end{equation}

\begin{equation}
	{\hat u_t} = {g_t} \odot {f_t} + (1 - {g_t}) \odot {u_t}
\end{equation}

#### decoding with attention and maxout pointer

在解码阶段，decoder 是另一个RNN，接受编码过后的 input 和之前解码完的词

\begin{equation}
	{d_t} = RNN^D(d_{t-1},y_{t-1})
\end{equation}

\begin{equation}
	p({y_t}|\{ {y_{ < t}}\} ) = softmax ({W^V}{d_t})
\end{equation}

$d_t$ 是 RNN 在时间 t 的 hidden state，$d_0$ 是  encoder 的最后一个 hidden state，通过 softmax 计算所有词的概率

##### attention
使用 luong attention mechanism

\begin{equation}
	{r_t} = \hat{U}^T{W^a}{d_t}
\end{equation}

\begin{equation}
	{a^d}_t = softmax ({r_t})
\end{equation}

\begin{equation}
	{c_t} = \hat U \cdot {a^d}_t
\end{equation}

\begin{equation}
	{\hat d}_t = \tanh ({W^b}[{d_t},{c_t}])
\end{equation}

$r_t$ 为 attention 分数，通过与 decoder state $d_t$ 结合生成一个新的 decoder state

##### copy/pointer

\begin{equation}
	sc^{copy}({y_t}) = \sum\limits_{k,{x_k} = {y_t}} {r_{t,k}} ,{y_t} \in \chi 
\end{equation}

\begin{equation}
	sc^{copy}({y_t}) =  - \inf ,otherwise
\end{equation}

复制机制是可以从文中复制词也可以从固定的单词表中生成
这篇文章用的 pointer mechanism 直接利用了注意力分数$r_t$