{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/img/apple-touch-icon.png","path":"img/apple-touch-icon.png","modified":0,"renderable":1},{"_id":"themes/next/source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"themes/next/source/img/police_beian.png","path":"img/police_beian.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/lazyload.js","path":"js/lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/next/source/img/default.png","path":"img/default.png","modified":0,"renderable":1},{"_id":"themes/next/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"source/images/pasted-1.png","path":"images/pasted-1.png","modified":0,"renderable":0},{"_id":"source/images/pasted-0.png","path":"images/pasted-0.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/prettify/tomorrow.min.css","path":"lib/prettify/tomorrow.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/prettify/github-v2.min.css","path":"lib/prettify/github-v2.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/prettify/tomorrow-night.min.css","path":"lib/prettify/tomorrow-night.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/prettify/tomorrow-night-eighties.min.css","path":"lib/prettify/tomorrow-night-eighties.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/img/morphling.jpg","path":"img/morphling.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"c74e52c872c223bd92755ba5eb2e046133a812b3","modified":1591770979091},{"_id":"themes/next/.gitignore","hash":"bd095eee271360a38772ee1a42d4f000fb722e5f","modified":1584106621069},{"_id":"themes/next/README.md","hash":"06f8d822773d23635fec3b8a3bce1ad223d2d1e0","modified":1584106621071},{"_id":"themes/next/README_en.md","hash":"5d5771a577e70e992016495defa88f14fafe2147","modified":1584106621071},{"_id":"themes/next/_config.yml","hash":"fef82ea7cf85a744fd7dfcbbee9fe8c142e727d6","modified":1584260963495},{"_id":"themes/next/_static_prefix.yml","hash":"23b2e42879977ab9e4615066b39eaa23eef0e86b","modified":1584106621072},{"_id":"themes/next/local-search.xml","hash":"6606c859dc91b1a216e1a2b9eb0d1ec98db5d98b","modified":1584106621083},{"_id":"themes/next/Changelog.md","hash":"7ce721f33ce90d9e81cdd6ba2699087d050d4189","modified":1584106621070},{"_id":"themes/next/LICENSE","hash":"77259cff2096bcf2974d2091a28302511b9103f4","modified":1584106621071},{"_id":"themes/next/.DS_Store","hash":"f16b7abd6e40069c163be847c5e1d2ce522d226b","modified":1584111024510},{"_id":"source/_posts/.DS_Store","hash":"18573e44811662261c92b01eb770573eb4f65ece","modified":1584459522722},{"_id":"source/_posts/论文阅读.md","hash":"d1fe7d374dff32d6d1f40a5cbcb3df0b90af9ea9","modified":1584456438789},{"_id":"source/_posts/Leetcode 22题 括号生成.md","hash":"c94bce8db7ae89a575957548d3a073b9a0a7a71a","modified":1584455435800},{"_id":"themes/next/languages/en.yml","hash":"370b61742ea947934c72331ad29a2f1ec10db70f","modified":1584106621073},{"_id":"themes/next/languages/ja.yml","hash":"5fb9d2233a62a737b455d18931c3cc5ea3264195","modified":1584106621073},{"_id":"themes/next/layout/404.ejs","hash":"6832dce539c45a5482ea5036ef5b384494c9ff97","modified":1584106621074},{"_id":"themes/next/languages/zh-CN.yml","hash":"ccc2ec0c56c5de49ca3b39da7f4d23edab59a942","modified":1584106621074},{"_id":"themes/next/layout/about.ejs","hash":"7d22cdc51483591cb43c8ac7afb8582fd470fc69","modified":1584106621082},{"_id":"themes/next/layout/archive.ejs","hash":"850ba52decd0684dd57a0440e1d3abc50e6de61e","modified":1584106621082},{"_id":"themes/next/layout/categories.ejs","hash":"c07a45f3024c0278eadfafd5e46be6bd516031a6","modified":1584106621082},{"_id":"themes/next/layout/category.ejs","hash":"9cc94d4044e1170ffff2f98d4842b0fc27bf168d","modified":1584106621082},{"_id":"themes/next/layout/index.ejs","hash":"9821c50f497a85508e990f29471e03441d373cda","modified":1584106621082},{"_id":"themes/next/layout/layout.ejs","hash":"482de42cb85bc99649aabe95a841e04df6d968aa","modified":1584106621082},{"_id":"themes/next/layout/links.ejs","hash":"3d180f5eacca89ade213463a9daa665ede07121d","modified":1584106621083},{"_id":"themes/next/layout/page.ejs","hash":"f31f0670fbaac644eb358624ae7546d3a0b6dd6d","modified":1584106621083},{"_id":"themes/next/layout/post.ejs","hash":"b1f223741d76ffad295b95a835c9aedb67b01797","modified":1584106621083},{"_id":"themes/next/layout/tag.ejs","hash":"f47a126f3848bcdf482f2ed9c7f396e65a30ef7d","modified":1584106621083},{"_id":"themes/next/layout/tags.ejs","hash":"4a98ae8b71590690f7802aa381d77ede2f0a600b","modified":1584106621083},{"_id":"themes/next/scripts/helpers.js","hash":"387f387ef75cfeeed9292cef64011763782222cd","modified":1584106621084},{"_id":"themes/next/scripts/lazyload.js","hash":"97fb654cae5be4a7ea9e479fc86483e754b15d68","modified":1584106621084},{"_id":"themes/next/scripts/local-search.js","hash":"1a62a6547e55415955e0085b12809ab208fddaa0","modified":1584106621084},{"_id":"themes/next/scripts/pages.js","hash":"f29c04ca871b23b78ec026535515f897667fe73a","modified":1584106621084},{"_id":"themes/next/scripts/merge-configs.js","hash":"56399a9e6d66914f21b8d956edcb64231314f96d","modified":1584106621084},{"_id":"themes/next/scripts/wordcount.js","hash":"154e838ba4f5b6e9510da7e618a63fe9b388bd3d","modified":1584106621086},{"_id":"themes/next/source/.DS_Store","hash":"66821433086aa4ead8216adbc34d0d8197395ccc","modified":1584111024516},{"_id":"themes/next/layout/_partial/beian.ejs","hash":"c6ff74cf9b284cc895461932c266d25be2df8e33","modified":1584106621075},{"_id":"themes/next/layout/_partial/busuanzi.ejs","hash":"cec5a8c84131039eb1f2c3df9310433421f87fb8","modified":1584106621075},{"_id":"themes/next/layout/_partial/css.ejs","hash":"b50fab687048e3b856a38487791697fc4b76da73","modified":1584106621077},{"_id":"themes/next/layout/_partial/footer.ejs","hash":"e24dba03acaf7873e51f11c2db73ccc26347a688","modified":1584106621077},{"_id":"themes/next/layout/_partial/head.ejs","hash":"ea023654308f1225316b613f5a74cf5ea1e1677e","modified":1584106621077},{"_id":"themes/next/layout/_partial/nav.ejs","hash":"c798fdce888c6f26790cd201e0962ce8d479a6fc","modified":1584106621078},{"_id":"themes/next/layout/_partial/paginator.ejs","hash":"e59408274d8060d535be8ab923284df2f8aa8506","modified":1584106621078},{"_id":"themes/next/layout/_partial/postjs.ejs","hash":"487738f5a94f80aca335e58313e3a2f51f708f1a","modified":1584106621081},{"_id":"themes/next/layout/_partial/scripts.ejs","hash":"05b1ae2933dc79fbe87cdc30d11b88cc1c95d37d","modified":1584106621081},{"_id":"themes/next/layout/_partial/search.ejs","hash":"cdd7919fa01f6ef7ccc09938d662ff3d77f5d999","modified":1584106621081},{"_id":"themes/next/layout/_partial/toc.ejs","hash":"6d74b47088972f1b116df7d910c3001724b35dba","modified":1584106621082},{"_id":"themes/next/scripts/utils/join-path.js","hash":"a2b4ffc665967d50ac4791cd724243a91e762338","modified":1584106621085},{"_id":"themes/next/source/css/main.styl","hash":"d5a8a59c8d1fd17d699a951e59c4ce9ae44c419d","modified":1584106621089},{"_id":"themes/next/source/img/apple-touch-icon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1584106621089},{"_id":"themes/next/source/img/.DS_Store","hash":"85a1ef5c1701949d1f5dff456e8a8fd5371e2848","modified":1584111402553},{"_id":"themes/next/source/img/favicon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1584106621090},{"_id":"themes/next/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1584106621090},{"_id":"themes/next/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1584106621091},{"_id":"themes/next/source/js/lazyload.js","hash":"1f09eb5e003dc9b34d272eac5b0c292f1eb94d80","modified":1584106621091},{"_id":"themes/next/source/js/local-search.js","hash":"66046f2ad85aa22fd64bbd0e968a498a2c5bfbaa","modified":1584106621091},{"_id":"themes/next/source/js/main.js","hash":"e1078845c70f201ae0d8d1e108c91fe04367f7ed","modified":1584106621091},{"_id":"themes/next/source/img/default.png","hash":"7bb2b8ee07db305bcadee2985b81b942027ae940","modified":1584106621090},{"_id":"themes/next/scripts/utils/lodash.js","hash":"cf01be92b3cd2bc69fc3f862270aa53afcba09ae","modified":1584106621086},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1584106621086},{"_id":"themes/next/source/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1584106621091},{"_id":"source/images/pasted-1.png","hash":"ab7b7677bc61bfbe316980bd9c4056b1bb429bc2","modified":1584360988533},{"_id":"themes/next/layout/_partial/comments/changyan.ejs","hash":"72af18c2d15a30ea9560b43b28b2e758abe892c0","modified":1584106621075},{"_id":"themes/next/layout/_partial/comments/gitalk.ejs","hash":"3b6d84f04018bcc5fcb06b4818bbf08414d4bc70","modified":1584106621076},{"_id":"themes/next/layout/_partial/comments/livere.ejs","hash":"01406b8bdfece518a30325cef9f2581ffcc9cb5e","modified":1584106621076},{"_id":"themes/next/layout/_partial/comments/disqus.ejs","hash":"6730d31dbbba74de721c550c1da8bb17a4b6ad21","modified":1584106621075},{"_id":"themes/next/layout/_partial/comments/utterances.ejs","hash":"cf8f5703344c6966d756c5bdaa0be70637a4f0e4","modified":1584106621076},{"_id":"themes/next/layout/_partial/comments/valine.ejs","hash":"a88e1c6c22f7f1347d1bf5e6c720517a22099e7b","modified":1584106621077},{"_id":"themes/next/layout/_partial/plugins/anchor.ejs","hash":"40032ca6af619716fa3479eeb9a5f47924d95126","modified":1584106621079},{"_id":"themes/next/layout/_partial/plugins/daovoice.ejs","hash":"cfc684ba48608abd25afd155ee373d9936bbe84e","modified":1584106621080},{"_id":"themes/next/layout/_partial/plugins/analytics.ejs","hash":"32e6e3f9167a9cc329c9e88c759cf1d4be91cb3a","modified":1584106621079},{"_id":"themes/next/layout/_partial/plugins/aplayer.ejs","hash":"87d6c0a17f8619261cee3e3f1abb981c30d03779","modified":1584106621079},{"_id":"themes/next/layout/_partial/plugins/fancybox.ejs","hash":"c1c53be5b317bb8e83157b18daa80dba3d28dd47","modified":1584106621080},{"_id":"themes/next/layout/_partial/plugins/math.ejs","hash":"2778826c66c442e33ac9f9c32bc6e9e1ec0ef8f5","modified":1584106621080},{"_id":"themes/next/layout/_partial/plugins/local-search.ejs","hash":"421ad423f4a44a9468253e1f117145a500fdec78","modified":1584106621080},{"_id":"themes/next/layout/_partial/plugins/mouse-click.ejs","hash":"eb19991199c201ceb103a6ef025e4dfd1e7dbfb7","modified":1584106621081},{"_id":"themes/next/layout/_partial/plugins/prettify.ejs","hash":"d835646f43d27cfa63c1d84c851a16cd1c49f970","modified":1584106621081},{"_id":"themes/next/layout/_partial/plugins/typed.ejs","hash":"2ee91a9782ffca316b12ab45a96a9bbafe57e5af","modified":1584106621081},{"_id":"themes/next/source/css/_functions/base.styl","hash":"37cdcd3c43cc89c6dc929f90bf342c6d33af0622","modified":1584106621086},{"_id":"themes/next/source/css/_pages/pages.styl","hash":"ff3913951b67695c2311b08ec6839d2c4005128c","modified":1584106621089},{"_id":"themes/next/source/css/_variables/base.styl","hash":"e4bae930859f0380e029cac9a51fd34e00892f91","modified":1584106621089},{"_id":"source/images/pasted-0.png","hash":"02e21f4486ee6375e93c685864e5d6af3400d12b","modified":1584178828966},{"_id":"themes/next/source/lib/prettify/tomorrow.min.css","hash":"ea61879c64ca73a5ea233b1315faf7f2fdfebca9","modified":1584106621092},{"_id":"themes/next/source/lib/prettify/github-v2.min.css","hash":"da1b8e6d4df1f044d12f461880e677d65dbbf2d3","modified":1584106621092},{"_id":"themes/next/source/lib/prettify/tomorrow-night.min.css","hash":"535256d676d247d3282e9a8ae2777c6f7df4fdc6","modified":1584106621092},{"_id":"themes/next/source/lib/prettify/tomorrow-night-eighties.min.css","hash":"a5f2102fc148359a92435b170f3bfb25e1221837","modified":1584106621092},{"_id":"themes/next/source/css/_pages/_archive/archive.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1584106621087},{"_id":"themes/next/source/css/_pages/_category/category.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1584106621087},{"_id":"themes/next/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1584106621088},{"_id":"themes/next/source/css/_pages/_category/categories.styl","hash":"43f49545fe2581338d971ecdf848942cc519e378","modified":1584106621087},{"_id":"themes/next/source/css/_pages/_about/about.styl","hash":"48e1ece7e22aeaf8ae45de5b2f318daf6dc2df49","modified":1584106621087},{"_id":"themes/next/source/css/_pages/_links/links.styl","hash":"efd09297f31e85e6ba6369414ca8583e86121f2c","modified":1584106621088},{"_id":"themes/next/source/css/_pages/_tag/tags.styl","hash":"8125a515d68bcbf53e373fdfa4be7d1270ba4607","modified":1584106621089},{"_id":"themes/next/source/css/_pages/_post/post.styl","hash":"d1763c3a49f1a4c29ca4fb0bb3468a1d24a96d89","modified":1584106621088},{"_id":"themes/next/source/css/_pages/_base/base.styl","hash":"2742f3ff71c28d66e0a5000d3ea1cace773f23e8","modified":1584106621087},{"_id":"themes/next/source/css/_pages/_index/index.styl","hash":"7a2307579400815d2f9dd0416f94d7ee6bbcf46a","modified":1584106621088},{"_id":"themes/next/source/img/morphling.jpg","hash":"039983618015ef34d882e001bae5165bd16e5e84","modified":1584111168756},{"_id":"public/local-search.xml","hash":"8b607757074f04d42742cbbb9017b440673b2c44","modified":1591774726819},{"_id":"public/2020/03/14/Leetcode 22题 括号生成/index.html","hash":"7cbe7d0c9448a68f4b6c22f9542f9a1042445e00","modified":1591774726819},{"_id":"public/categories/leetcode/index.html","hash":"da4b70922cac28d92718814ac88e985b58323d21","modified":1591774726819},{"_id":"public/categories/论文阅读/index.html","hash":"993084c29368e2804bfbee5e3d07d2633c05d077","modified":1591774726819},{"_id":"public/archives/2020/index.html","hash":"09992b8af42038af7a5020ff0b3798216acefe11","modified":1591774726819},{"_id":"public/archives/index.html","hash":"09992b8af42038af7a5020ff0b3798216acefe11","modified":1591774726819},{"_id":"public/archives/2020/03/index.html","hash":"09992b8af42038af7a5020ff0b3798216acefe11","modified":1591774726819},{"_id":"public/index.html","hash":"b742bdeb3cc32c87acf6cf399703666c87d05305","modified":1591774726819},{"_id":"public/tags/回溯/index.html","hash":"f10a01b21419021a09ff600a583a4bd36ce636ee","modified":1591774726819},{"_id":"public/tags/问题生成/index.html","hash":"f178d9d9c4787e99239b81416eceefb698c071bc","modified":1591774726819},{"_id":"public/tags/index.html","hash":"fcb06c75cdefa6c1fd5455bb2f900c060ec3eb6d","modified":1591774726819},{"_id":"public/404.html","hash":"72659a03da9c05b48fe65e49a4e66222e6921249","modified":1591774726819},{"_id":"public/categories/index.html","hash":"a1ac2ab854e70d8942d59e107b1ab0661221e022","modified":1591774726819},{"_id":"public/links/index.html","hash":"1b2a692a8185a3d1d798f6abaef1597fdb27aa75","modified":1591774726819},{"_id":"public/2020/03/17/论文阅读/index.html","hash":"1c1b4c08e716724a34b5feb8b4019f45ad48182b","modified":1591774726819},{"_id":"public/img/apple-touch-icon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1591774726819},{"_id":"public/img/favicon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1591774726819},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1591774726819},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1591774726819},{"_id":"public/img/default.png","hash":"7bb2b8ee07db305bcadee2985b81b942027ae940","modified":1591774726819},{"_id":"public/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1591774726819},{"_id":"public/js/lazyload.js","hash":"1f09eb5e003dc9b34d272eac5b0c292f1eb94d80","modified":1591774726819},{"_id":"public/js/local-search.js","hash":"66046f2ad85aa22fd64bbd0e968a498a2c5bfbaa","modified":1591774726819},{"_id":"public/js/main.js","hash":"e1078845c70f201ae0d8d1e108c91fe04367f7ed","modified":1591774726819},{"_id":"public/lib/prettify/tomorrow.min.css","hash":"ea61879c64ca73a5ea233b1315faf7f2fdfebca9","modified":1591774726819},{"_id":"public/lib/prettify/github-v2.min.css","hash":"da1b8e6d4df1f044d12f461880e677d65dbbf2d3","modified":1591774726819},{"_id":"public/lib/prettify/tomorrow-night.min.css","hash":"535256d676d247d3282e9a8ae2777c6f7df4fdc6","modified":1591774726819},{"_id":"public/lib/prettify/tomorrow-night-eighties.min.css","hash":"a5f2102fc148359a92435b170f3bfb25e1221837","modified":1591774726819},{"_id":"public/css/main.css","hash":"cd754b71ef6b8a04c9443ee2541e9718deb95c07","modified":1591774726819},{"_id":"public/images/pasted-1.png","hash":"ab7b7677bc61bfbe316980bd9c4056b1bb429bc2","modified":1591774726819},{"_id":"public/images/pasted-0.png","hash":"02e21f4486ee6375e93c685864e5d6af3400d12b","modified":1591774726819},{"_id":"public/img/morphling.jpg","hash":"039983618015ef34d882e001bae5165bd16e5e84","modified":1591774726819}],"Category":[{"name":"论文阅读","_id":"ckb91ko3u0001y5743qczhn27"},{"name":"leetcode","_id":"ckb91ko4j0006y574dn684sxw"}],"Data":[],"Page":[],"Post":[{"title":"论文阅读 Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks","author":"Morphling","date":"2020-03-17T14:14:00.000Z","_content":"**EMNLP 2018**\n### Abstract \n这篇主要针对的任务是答案为文章中片段的问题生成，问题生成可以用在问答系统和语音助手以及教育等领域上。现存的模型大多数都是以一或两句作为输入，长文本仍然在 seq2seq 模型表现不是很理想（以整段作为输入）。这篇文章提出了 maxout pointer mechansim with gated self-attention encoder 去处理长文本的问题。BLUE-4 分数达到了 **16.3**。\n\n---\n### Introduction\nQG 的应用场景：QnA and conversation systems, education (H&S， 2010)\nQG 的资料集与阅读理解的资料集很相似，比如 SQuAD 和 MS MARCO。\n这篇文章处理的 QG 类型是 answer-aware，输入为文章和答案，输出是以答案为目标的问题。答案是在文章中的一段。\n(Du Et al., 2017) 指出大概在 SQuAD 20% 的问题需要整个段落的信息来回答，但是整个段落包含了很多无关的信息，所以怎么利用好在段落中有关的信息是一个挑战。在此之前的 para-level 模型表现都不是很理想。\n这篇文章提出了一个 seq2seq attenion model， 包含了 maxout pointer mechansim 和 gated self-attention 来处理 para-level 的输入。\n\n---\n### Model\n![upload successful](/images/pasted-1.png)\n#### Problem definition\nP: 文章(可为句子或者段落)\nA: 答案\n任务是找到 $\\overline{Q}$:\n\n\\begin{equation}\n\t\\overline{Q} = \\mathop{\\arg\\max}_{Q} Prob(Q|P, A)\n\\end{equation}\n\nQ可能是来自于文章或者字典\n#### Passage and Answer Encoding\n##### encoding process\n\n\\begin{equation}\n\tu_{t} = RNN^{E}(u_{t-1}, [e_{t}, m_{t}])\n\\end{equation}\n\n$u_{t}$ 表示 RNN 在时间 t 的隐藏状态，$e_{t}$ 表示 $x_{t}$ 在 文章 P 的 word embedding，$m_{t}$ 表示该词是否在答案中，$[a, b]$ 为 concatenation a 和 b。这篇文章把该方法称为 answer tagging，目的是为了可以生成与答案更相关的问题。\n如果 $RNN^{e}$ 是双向的话，$ U = {[\\overrightarrow{u_{t}},\\overleftarrow{u_{t}}]} $\n\n##### gated self-attention\n**该部分内容参考 [xinze](https://www.cnblogs.com/LuoboLiam/p/11688786.html)**\ngated self-attention 设计用来整合整个文章的信息还有嵌入文章内部的依赖关系，在每一时间步中优化文章和答案的嵌入表示。\n分为两步，一、$a_{t}^{s}$ 是段落中所有 encode 的单词对当前 $t$ 时刻所对应单词的之间的依赖关系的系数，$U$ 表示从1到最后时刻所有的 hidden state 组成的矩阵，即表示 passage-answer；${s_t}$ 表示段落中所有 encode 的单词对当前$t$时刻所对应单词的之间的依赖关系，也是 self matching 的表示。类似于self-attention，主要目的是计算文章中不同单词对于生成问题的重要性（相关性）\n\n\\begin{equation}\n\ta_{t}^{s} = softmax(U^{T}W^{s}u_{t})\n\\end{equation}\n\n\\begin{equation}\n\ts_{t} = U·a_{t}^{s}\n\\end{equation}\n\n\n二、${f_t}$ 表示新的包含 self matching 信息的 passage-answer 表示，${g_t}$ 表示一个可学习的门控单元，最后，${\\hat u_t}$ 表示新的 passage-answer 表示，用来喂给decoder。gated attention 可以专注于 answer 与当前段落之间的关系。\n\n\\begin{equation}\n\t{f_t} = \\tanh ({W^f}[{u_t},{s_t}])\n\\end{equation}\n\n\\begin{equation}\n\t{g_t} = sigmoid({W^g}[{u_t},{s_t}])\n\\end{equation}\n\n\\begin{equation}\n\t{\\hat u_t} = {g_t} \\odot {f_t} + (1 - {g_t}) \\odot {u_t}\n\\end{equation}\n\n#### decoding with attention and maxout pointer\n\n在解码阶段，decoder 是另一个RNN，接受编码过后的 input 和之前解码完的词\n\n\\begin{equation}\n\t{d_t} = RNN^D(d_{t-1},y_{t-1})\n\\end{equation}\n\n\\begin{equation}\n\tp({y_t}|\\{ {y_{ < t}}\\} ) = softmax ({W^V}{d_t})\n\\end{equation}\n\n$d_t$ 是 RNN 在时间 t 的 hidden state，$d_0$ 是  encoder 的最后一个 hidden state，通过 softmax 计算所有词的概率\n\n##### attention\n使用 luong attention mechanism\n\n\\begin{equation}\n\t{r_t} = \\hat{U}^T{W^a}{d_t}\n\\end{equation}\n\n\\begin{equation}\n\t{a^d}_t = softmax ({r_t})\n\\end{equation}\n\n\\begin{equation}\n\t{c_t} = \\hat U \\cdot {a^d}_t\n\\end{equation}\n\n\\begin{equation}\n\t{\\hat d}_t = \\tanh ({W^b}[{d_t},{c_t}])\n\\end{equation}\n\n$r_t$ 为 attention 分数，通过与 decoder state $d_t$ 结合生成一个新的 decoder state\n\n##### copy/pointer\n\n\\begin{equation}\n\tsc^{copy}({y_t}) = \\sum\\limits_{k,{x_k} = {y_t}} {r_{t,k}} ,{y_t} \\in \\chi \n\\end{equation}\n\n\\begin{equation}\n\tsc^{copy}({y_t}) =  - \\inf ,otherwise\n\\end{equation}\n\n复制机制是可以从文中复制词也可以从固定的单词表中生成\n这篇文章用的 pointer mechanism 直接利用了注意力分数$r_t$","source":"_posts/论文阅读.md","raw":"title: >-\n  论文阅读 Paragraph-level Neural Question Generation with Maxout Pointer and Gated\n  Self-attention Networks\nauthor: Morphling\ntags:\n  - 问题生成\ncategories:\n  - 论文阅读\ndate: 2020-03-17 22:14:00\n---\n**EMNLP 2018**\n### Abstract \n这篇主要针对的任务是答案为文章中片段的问题生成，问题生成可以用在问答系统和语音助手以及教育等领域上。现存的模型大多数都是以一或两句作为输入，长文本仍然在 seq2seq 模型表现不是很理想（以整段作为输入）。这篇文章提出了 maxout pointer mechansim with gated self-attention encoder 去处理长文本的问题。BLUE-4 分数达到了 **16.3**。\n\n---\n### Introduction\nQG 的应用场景：QnA and conversation systems, education (H&S， 2010)\nQG 的资料集与阅读理解的资料集很相似，比如 SQuAD 和 MS MARCO。\n这篇文章处理的 QG 类型是 answer-aware，输入为文章和答案，输出是以答案为目标的问题。答案是在文章中的一段。\n(Du Et al., 2017) 指出大概在 SQuAD 20% 的问题需要整个段落的信息来回答，但是整个段落包含了很多无关的信息，所以怎么利用好在段落中有关的信息是一个挑战。在此之前的 para-level 模型表现都不是很理想。\n这篇文章提出了一个 seq2seq attenion model， 包含了 maxout pointer mechansim 和 gated self-attention 来处理 para-level 的输入。\n\n---\n### Model\n![upload successful](/images/pasted-1.png)\n#### Problem definition\nP: 文章(可为句子或者段落)\nA: 答案\n任务是找到 $\\overline{Q}$:\n\n\\begin{equation}\n\t\\overline{Q} = \\mathop{\\arg\\max}_{Q} Prob(Q|P, A)\n\\end{equation}\n\nQ可能是来自于文章或者字典\n#### Passage and Answer Encoding\n##### encoding process\n\n\\begin{equation}\n\tu_{t} = RNN^{E}(u_{t-1}, [e_{t}, m_{t}])\n\\end{equation}\n\n$u_{t}$ 表示 RNN 在时间 t 的隐藏状态，$e_{t}$ 表示 $x_{t}$ 在 文章 P 的 word embedding，$m_{t}$ 表示该词是否在答案中，$[a, b]$ 为 concatenation a 和 b。这篇文章把该方法称为 answer tagging，目的是为了可以生成与答案更相关的问题。\n如果 $RNN^{e}$ 是双向的话，$ U = {[\\overrightarrow{u_{t}},\\overleftarrow{u_{t}}]} $\n\n##### gated self-attention\n**该部分内容参考 [xinze](https://www.cnblogs.com/LuoboLiam/p/11688786.html)**\ngated self-attention 设计用来整合整个文章的信息还有嵌入文章内部的依赖关系，在每一时间步中优化文章和答案的嵌入表示。\n分为两步，一、$a_{t}^{s}$ 是段落中所有 encode 的单词对当前 $t$ 时刻所对应单词的之间的依赖关系的系数，$U$ 表示从1到最后时刻所有的 hidden state 组成的矩阵，即表示 passage-answer；${s_t}$ 表示段落中所有 encode 的单词对当前$t$时刻所对应单词的之间的依赖关系，也是 self matching 的表示。类似于self-attention，主要目的是计算文章中不同单词对于生成问题的重要性（相关性）\n\n\\begin{equation}\n\ta_{t}^{s} = softmax(U^{T}W^{s}u_{t})\n\\end{equation}\n\n\\begin{equation}\n\ts_{t} = U·a_{t}^{s}\n\\end{equation}\n\n\n二、${f_t}$ 表示新的包含 self matching 信息的 passage-answer 表示，${g_t}$ 表示一个可学习的门控单元，最后，${\\hat u_t}$ 表示新的 passage-answer 表示，用来喂给decoder。gated attention 可以专注于 answer 与当前段落之间的关系。\n\n\\begin{equation}\n\t{f_t} = \\tanh ({W^f}[{u_t},{s_t}])\n\\end{equation}\n\n\\begin{equation}\n\t{g_t} = sigmoid({W^g}[{u_t},{s_t}])\n\\end{equation}\n\n\\begin{equation}\n\t{\\hat u_t} = {g_t} \\odot {f_t} + (1 - {g_t}) \\odot {u_t}\n\\end{equation}\n\n#### decoding with attention and maxout pointer\n\n在解码阶段，decoder 是另一个RNN，接受编码过后的 input 和之前解码完的词\n\n\\begin{equation}\n\t{d_t} = RNN^D(d_{t-1},y_{t-1})\n\\end{equation}\n\n\\begin{equation}\n\tp({y_t}|\\{ {y_{ < t}}\\} ) = softmax ({W^V}{d_t})\n\\end{equation}\n\n$d_t$ 是 RNN 在时间 t 的 hidden state，$d_0$ 是  encoder 的最后一个 hidden state，通过 softmax 计算所有词的概率\n\n##### attention\n使用 luong attention mechanism\n\n\\begin{equation}\n\t{r_t} = \\hat{U}^T{W^a}{d_t}\n\\end{equation}\n\n\\begin{equation}\n\t{a^d}_t = softmax ({r_t})\n\\end{equation}\n\n\\begin{equation}\n\t{c_t} = \\hat U \\cdot {a^d}_t\n\\end{equation}\n\n\\begin{equation}\n\t{\\hat d}_t = \\tanh ({W^b}[{d_t},{c_t}])\n\\end{equation}\n\n$r_t$ 为 attention 分数，通过与 decoder state $d_t$ 结合生成一个新的 decoder state\n\n##### copy/pointer\n\n\\begin{equation}\n\tsc^{copy}({y_t}) = \\sum\\limits_{k,{x_k} = {y_t}} {r_{t,k}} ,{y_t} \\in \\chi \n\\end{equation}\n\n\\begin{equation}\n\tsc^{copy}({y_t}) =  - \\inf ,otherwise\n\\end{equation}\n\n复制机制是可以从文中复制词也可以从固定的单词表中生成\n这篇文章用的 pointer mechanism 直接利用了注意力分数$r_t$","slug":"论文阅读","published":1,"updated":"2020-03-17T14:47:18.789Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb91ko3o0000y574buwpfr0f","content":"<p><strong>EMNLP 2018</strong></p>\n<h3 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h3><p>这篇主要针对的任务是答案为文章中片段的问题生成，问题生成可以用在问答系统和语音助手以及教育等领域上。现存的模型大多数都是以一或两句作为输入，长文本仍然在 seq2seq 模型表现不是很理想（以整段作为输入）。这篇文章提出了 maxout pointer mechansim with gated self-attention encoder 去处理长文本的问题。BLUE-4 分数达到了 <strong>16.3</strong>。</p>\n<hr>\n<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>QG 的应用场景：QnA and conversation systems, education (H&amp;S， 2010)<br>QG 的资料集与阅读理解的资料集很相似，比如 SQuAD 和 MS MARCO。<br>这篇文章处理的 QG 类型是 answer-aware，输入为文章和答案，输出是以答案为目标的问题。答案是在文章中的一段。<br>(Du Et al., 2017) 指出大概在 SQuAD 20% 的问题需要整个段落的信息来回答，但是整个段落包含了很多无关的信息，所以怎么利用好在段落中有关的信息是一个挑战。在此之前的 para-level 模型表现都不是很理想。<br>这篇文章提出了一个 seq2seq attenion model， 包含了 maxout pointer mechansim 和 gated self-attention 来处理 para-level 的输入。</p>\n<hr>\n<h3 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p><img src=\"/images/pasted-1.png\" srcset=\"/img/loading.gif\" alt=\"upload successful\"></p>\n<h4 id=\"Problem-definition\"><a href=\"#Problem-definition\" class=\"headerlink\" title=\"Problem definition\"></a>Problem definition</h4><p>P: 文章(可为句子或者段落)<br>A: 答案<br>任务是找到 $\\overline{Q}$:</p>\n<p>\\begin{equation}<br>    \\overline{Q} = \\mathop{\\arg\\max}_{Q} Prob(Q|P, A)<br>\\end{equation}</p>\n<p>Q可能是来自于文章或者字典</p>\n<h4 id=\"Passage-and-Answer-Encoding\"><a href=\"#Passage-and-Answer-Encoding\" class=\"headerlink\" title=\"Passage and Answer Encoding\"></a>Passage and Answer Encoding</h4><h5 id=\"encoding-process\"><a href=\"#encoding-process\" class=\"headerlink\" title=\"encoding process\"></a>encoding process</h5><p>\\begin{equation}<br>    u_{t} = RNN^{E}(u_{t-1}, [e_{t}, m_{t}])<br>\\end{equation}</p>\n<p>$u_{t}$ 表示 RNN 在时间 t 的隐藏状态，$e_{t}$ 表示 $x_{t}$ 在 文章 P 的 word embedding，$m_{t}$ 表示该词是否在答案中，$[a, b]$ 为 concatenation a 和 b。这篇文章把该方法称为 answer tagging，目的是为了可以生成与答案更相关的问题。<br>如果 $RNN^{e}$ 是双向的话，$ U = {[\\overrightarrow{u_{t}},\\overleftarrow{u_{t}}]} $</p>\n<h5 id=\"gated-self-attention\"><a href=\"#gated-self-attention\" class=\"headerlink\" title=\"gated self-attention\"></a>gated self-attention</h5><p><strong>该部分内容参考 <a href=\"https://www.cnblogs.com/LuoboLiam/p/11688786.html\" target=\"_blank\" rel=\"noopener\">xinze</a></strong><br>gated self-attention 设计用来整合整个文章的信息还有嵌入文章内部的依赖关系，在每一时间步中优化文章和答案的嵌入表示。<br>分为两步，一、$a_{t}^{s}$ 是段落中所有 encode 的单词对当前 $t$ 时刻所对应单词的之间的依赖关系的系数，$U$ 表示从1到最后时刻所有的 hidden state 组成的矩阵，即表示 passage-answer；${s_t}$ 表示段落中所有 encode 的单词对当前$t$时刻所对应单词的之间的依赖关系，也是 self matching 的表示。类似于self-attention，主要目的是计算文章中不同单词对于生成问题的重要性（相关性）</p>\n<p>\\begin{equation}<br>    a_{t}^{s} = softmax(U^{T}W^{s}u_{t})<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    s_{t} = U·a_{t}^{s}<br>\\end{equation}</p>\n<p>二、${f_t}$ 表示新的包含 self matching 信息的 passage-answer 表示，${g_t}$ 表示一个可学习的门控单元，最后，${\\hat u_t}$ 表示新的 passage-answer 表示，用来喂给decoder。gated attention 可以专注于 answer 与当前段落之间的关系。</p>\n<p>\\begin{equation}<br>    {f_t} = \\tanh ({W^f}[{u_t},{s_t}])<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {g_t} = sigmoid({W^g}[{u_t},{s_t}])<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {\\hat u_t} = {g_t} \\odot {f_t} + (1 - {g_t}) \\odot {u_t}<br>\\end{equation}</p>\n<h4 id=\"decoding-with-attention-and-maxout-pointer\"><a href=\"#decoding-with-attention-and-maxout-pointer\" class=\"headerlink\" title=\"decoding with attention and maxout pointer\"></a>decoding with attention and maxout pointer</h4><p>在解码阶段，decoder 是另一个RNN，接受编码过后的 input 和之前解码完的词</p>\n<p>\\begin{equation}<br>    {d_t} = RNN^D(d_{t-1},y_{t-1})<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    p({y_t}|{ {y_{ &lt; t}}} ) = softmax ({W^V}{d_t})<br>\\end{equation}</p>\n<p>$d_t$ 是 RNN 在时间 t 的 hidden state，$d_0$ 是  encoder 的最后一个 hidden state，通过 softmax 计算所有词的概率</p>\n<h5 id=\"attention\"><a href=\"#attention\" class=\"headerlink\" title=\"attention\"></a>attention</h5><p>使用 luong attention mechanism</p>\n<p>\\begin{equation}<br>    {r_t} = \\hat{U}^T{W^a}{d_t}<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {a^d}_t = softmax ({r_t})<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {c_t} = \\hat U \\cdot {a^d}_t<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {\\hat d}_t = \\tanh ({W^b}[{d_t},{c_t}])<br>\\end{equation}</p>\n<p>$r_t$ 为 attention 分数，通过与 decoder state $d_t$ 结合生成一个新的 decoder state</p>\n<h5 id=\"copy-pointer\"><a href=\"#copy-pointer\" class=\"headerlink\" title=\"copy/pointer\"></a>copy/pointer</h5><p>\\begin{equation}<br>    sc^{copy}({y_t}) = \\sum\\limits_{k,{x_k} = {y_t}} {r_{t,k}} ,{y_t} \\in \\chi<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    sc^{copy}({y_t}) =  - \\inf ,otherwise<br>\\end{equation}</p>\n<p>复制机制是可以从文中复制词也可以从固定的单词表中生成<br>这篇文章用的 pointer mechanism 直接利用了注意力分数$r_t$</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>EMNLP 2018</strong></p>\n<h3 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h3><p>这篇主要针对的任务是答案为文章中片段的问题生成，问题生成可以用在问答系统和语音助手以及教育等领域上。现存的模型大多数都是以一或两句作为输入，长文本仍然在 seq2seq 模型表现不是很理想（以整段作为输入）。这篇文章提出了 maxout pointer mechansim with gated self-attention encoder 去处理长文本的问题。BLUE-4 分数达到了 <strong>16.3</strong>。</p>\n<hr>\n<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>QG 的应用场景：QnA and conversation systems, education (H&amp;S， 2010)<br>QG 的资料集与阅读理解的资料集很相似，比如 SQuAD 和 MS MARCO。<br>这篇文章处理的 QG 类型是 answer-aware，输入为文章和答案，输出是以答案为目标的问题。答案是在文章中的一段。<br>(Du Et al., 2017) 指出大概在 SQuAD 20% 的问题需要整个段落的信息来回答，但是整个段落包含了很多无关的信息，所以怎么利用好在段落中有关的信息是一个挑战。在此之前的 para-level 模型表现都不是很理想。<br>这篇文章提出了一个 seq2seq attenion model， 包含了 maxout pointer mechansim 和 gated self-attention 来处理 para-level 的输入。</p>\n<hr>\n<h3 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p><img src=\"/images/pasted-1.png\" srcset=\"/img/loading.gif\" alt=\"upload successful\"></p>\n<h4 id=\"Problem-definition\"><a href=\"#Problem-definition\" class=\"headerlink\" title=\"Problem definition\"></a>Problem definition</h4><p>P: 文章(可为句子或者段落)<br>A: 答案<br>任务是找到 $\\overline{Q}$:</p>\n<p>\\begin{equation}<br>    \\overline{Q} = \\mathop{\\arg\\max}_{Q} Prob(Q|P, A)<br>\\end{equation}</p>\n<p>Q可能是来自于文章或者字典</p>\n<h4 id=\"Passage-and-Answer-Encoding\"><a href=\"#Passage-and-Answer-Encoding\" class=\"headerlink\" title=\"Passage and Answer Encoding\"></a>Passage and Answer Encoding</h4><h5 id=\"encoding-process\"><a href=\"#encoding-process\" class=\"headerlink\" title=\"encoding process\"></a>encoding process</h5><p>\\begin{equation}<br>    u_{t} = RNN^{E}(u_{t-1}, [e_{t}, m_{t}])<br>\\end{equation}</p>\n<p>$u_{t}$ 表示 RNN 在时间 t 的隐藏状态，$e_{t}$ 表示 $x_{t}$ 在 文章 P 的 word embedding，$m_{t}$ 表示该词是否在答案中，$[a, b]$ 为 concatenation a 和 b。这篇文章把该方法称为 answer tagging，目的是为了可以生成与答案更相关的问题。<br>如果 $RNN^{e}$ 是双向的话，$ U = {[\\overrightarrow{u_{t}},\\overleftarrow{u_{t}}]} $</p>\n<h5 id=\"gated-self-attention\"><a href=\"#gated-self-attention\" class=\"headerlink\" title=\"gated self-attention\"></a>gated self-attention</h5><p><strong>该部分内容参考 <a href=\"https://www.cnblogs.com/LuoboLiam/p/11688786.html\" target=\"_blank\" rel=\"noopener\">xinze</a></strong><br>gated self-attention 设计用来整合整个文章的信息还有嵌入文章内部的依赖关系，在每一时间步中优化文章和答案的嵌入表示。<br>分为两步，一、$a_{t}^{s}$ 是段落中所有 encode 的单词对当前 $t$ 时刻所对应单词的之间的依赖关系的系数，$U$ 表示从1到最后时刻所有的 hidden state 组成的矩阵，即表示 passage-answer；${s_t}$ 表示段落中所有 encode 的单词对当前$t$时刻所对应单词的之间的依赖关系，也是 self matching 的表示。类似于self-attention，主要目的是计算文章中不同单词对于生成问题的重要性（相关性）</p>\n<p>\\begin{equation}<br>    a_{t}^{s} = softmax(U^{T}W^{s}u_{t})<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    s_{t} = U·a_{t}^{s}<br>\\end{equation}</p>\n<p>二、${f_t}$ 表示新的包含 self matching 信息的 passage-answer 表示，${g_t}$ 表示一个可学习的门控单元，最后，${\\hat u_t}$ 表示新的 passage-answer 表示，用来喂给decoder。gated attention 可以专注于 answer 与当前段落之间的关系。</p>\n<p>\\begin{equation}<br>    {f_t} = \\tanh ({W^f}[{u_t},{s_t}])<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {g_t} = sigmoid({W^g}[{u_t},{s_t}])<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {\\hat u_t} = {g_t} \\odot {f_t} + (1 - {g_t}) \\odot {u_t}<br>\\end{equation}</p>\n<h4 id=\"decoding-with-attention-and-maxout-pointer\"><a href=\"#decoding-with-attention-and-maxout-pointer\" class=\"headerlink\" title=\"decoding with attention and maxout pointer\"></a>decoding with attention and maxout pointer</h4><p>在解码阶段，decoder 是另一个RNN，接受编码过后的 input 和之前解码完的词</p>\n<p>\\begin{equation}<br>    {d_t} = RNN^D(d_{t-1},y_{t-1})<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    p({y_t}|{ {y_{ &lt; t}}} ) = softmax ({W^V}{d_t})<br>\\end{equation}</p>\n<p>$d_t$ 是 RNN 在时间 t 的 hidden state，$d_0$ 是  encoder 的最后一个 hidden state，通过 softmax 计算所有词的概率</p>\n<h5 id=\"attention\"><a href=\"#attention\" class=\"headerlink\" title=\"attention\"></a>attention</h5><p>使用 luong attention mechanism</p>\n<p>\\begin{equation}<br>    {r_t} = \\hat{U}^T{W^a}{d_t}<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {a^d}_t = softmax ({r_t})<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {c_t} = \\hat U \\cdot {a^d}_t<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    {\\hat d}_t = \\tanh ({W^b}[{d_t},{c_t}])<br>\\end{equation}</p>\n<p>$r_t$ 为 attention 分数，通过与 decoder state $d_t$ 结合生成一个新的 decoder state</p>\n<h5 id=\"copy-pointer\"><a href=\"#copy-pointer\" class=\"headerlink\" title=\"copy/pointer\"></a>copy/pointer</h5><p>\\begin{equation}<br>    sc^{copy}({y_t}) = \\sum\\limits_{k,{x_k} = {y_t}} {r_{t,k}} ,{y_t} \\in \\chi<br>\\end{equation}</p>\n<p>\\begin{equation}<br>    sc^{copy}({y_t}) =  - \\inf ,otherwise<br>\\end{equation}</p>\n<p>复制机制是可以从文中复制词也可以从固定的单词表中生成<br>这篇文章用的 pointer mechanism 直接利用了注意力分数$r_t$</p>\n"},{"title":"Leetcode 22题 括号生成","author":"Morphling","date":"2020-03-14T06:42:00.000Z","_content":"#### 内容来自 [liweiwei1419](https://leetcode-cn.com/problems/generate-parentheses/solution/hui-su-suan-fa-by-liweiwei1419)\n> 1. 回溯算法本质上是树形问题的深度优先遍历，深度优先遍历有回退的过程，就需要状态重置 (就是回溯) ；\n> 2. 但如果状态变量是字符串的时候，因为字符串的不可变性，在拼接过程中会产生新的字符串，因此每一个状态 (树中的每一个节点) 都是新的字符串，可以不用显式回溯；\n> 3. 深度优先遍历用递归去实现的原因：\n   (1) 树形结构本身就是递归定义的\n   (2) 深度优先遍历要用到 stack, 但可以用函数的栈，把需要的节点状态变量设置为函数的参数，这样就不用再写一个节点类去完成深度优先遍历。\n---\n![upload successful](/images/pasted-0.png)\n---\n{% codeblock [深度优先遍历 做减法] [lang:python] [url] [link text] %}\nclass Solution:\n    def generateParenthesis(self, n: int) -> List[str]:\n        cut_str = \"\"\n        res = []\n        def dfs(cur_str, left, right): # 输入当前字符串内容, 剩余左括号数量, 剩余右括号数量\n        \tif left == 0 and right == 0:\n            \tres.append(cur_str)\n                return\n            if left > right:\n            \treturn\n            if left > 0:\n            \tdfs(cur_str + \"(\", left - 1, right)\n            if right > 0:\n            \tdfs(cur_str + \")\", ledt, right - 1)\n        dfs(cur_str, n, n)\n        return res\n{% endcodeblock %}","source":"_posts/Leetcode 22题 括号生成.md","raw":"title: Leetcode 22题 括号生成\nauthor: Morphling\ntags:\n  - 回溯\ncategories:\n  - leetcode\ndate: 2020-03-14 14:42:00\n---\n#### 内容来自 [liweiwei1419](https://leetcode-cn.com/problems/generate-parentheses/solution/hui-su-suan-fa-by-liweiwei1419)\n> 1. 回溯算法本质上是树形问题的深度优先遍历，深度优先遍历有回退的过程，就需要状态重置 (就是回溯) ；\n> 2. 但如果状态变量是字符串的时候，因为字符串的不可变性，在拼接过程中会产生新的字符串，因此每一个状态 (树中的每一个节点) 都是新的字符串，可以不用显式回溯；\n> 3. 深度优先遍历用递归去实现的原因：\n   (1) 树形结构本身就是递归定义的\n   (2) 深度优先遍历要用到 stack, 但可以用函数的栈，把需要的节点状态变量设置为函数的参数，这样就不用再写一个节点类去完成深度优先遍历。\n---\n![upload successful](/images/pasted-0.png)\n---\n{% codeblock [深度优先遍历 做减法] [lang:python] [url] [link text] %}\nclass Solution:\n    def generateParenthesis(self, n: int) -> List[str]:\n        cut_str = \"\"\n        res = []\n        def dfs(cur_str, left, right): # 输入当前字符串内容, 剩余左括号数量, 剩余右括号数量\n        \tif left == 0 and right == 0:\n            \tres.append(cur_str)\n                return\n            if left > right:\n            \treturn\n            if left > 0:\n            \tdfs(cur_str + \"(\", left - 1, right)\n            if right > 0:\n            \tdfs(cur_str + \")\", ledt, right - 1)\n        dfs(cur_str, n, n)\n        return res\n{% endcodeblock %}","slug":"Leetcode 22题 括号生成","published":1,"updated":"2020-03-17T14:30:35.800Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckb91ko4i0005y574a4z111un","content":"<h4 id=\"内容来自-liweiwei1419\"><a href=\"#内容来自-liweiwei1419\" class=\"headerlink\" title=\"内容来自 liweiwei1419\"></a>内容来自 <a href=\"https://leetcode-cn.com/problems/generate-parentheses/solution/hui-su-suan-fa-by-liweiwei1419\" target=\"_blank\" rel=\"noopener\">liweiwei1419</a></h4><blockquote>\n<ol>\n<li>回溯算法本质上是树形问题的深度优先遍历，深度优先遍历有回退的过程，就需要状态重置 (就是回溯) ；</li>\n<li>但如果状态变量是字符串的时候，因为字符串的不可变性，在拼接过程中会产生新的字符串，因此每一个状态 (树中的每一个节点) 都是新的字符串，可以不用显式回溯；</li>\n<li>深度优先遍历用递归去实现的原因：<br>(1) 树形结构本身就是递归定义的<br>(2) 深度优先遍历要用到 stack, 但可以用函数的栈，把需要的节点状态变量设置为函数的参数，这样就不用再写一个节点类去完成深度优先遍历。</li>\n</ol>\n</blockquote>\n<hr>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><img src=\"/images/pasted-0.png\" srcset=\"/img/loading.gif\" alt=\"upload successful\"></h2><pre><code>class Solution:\n    def generateParenthesis(self, n: int) -&gt; List[str]:\n        cut_str &#x3D; &quot;&quot;\n        res &#x3D; []\n        def dfs(cur_str, left, right): # 输入当前字符串内容, 剩余左括号数量, 剩余右括号数量\n        \tif left &#x3D;&#x3D; 0 and right &#x3D;&#x3D; 0:\n            \tres.append(cur_str)\n                return\n            if left &gt; right:\n            \treturn\n            if left &gt; 0:\n            \tdfs(cur_str + &quot;(&quot;, left - 1, right)\n            if right &gt; 0:\n            \tdfs(cur_str + &quot;)&quot;, ledt, right - 1)\n        dfs(cur_str, n, n)\n        return res</code></pre>","site":{"data":{}},"excerpt":"","more":"<h4 id=\"内容来自-liweiwei1419\"><a href=\"#内容来自-liweiwei1419\" class=\"headerlink\" title=\"内容来自 liweiwei1419\"></a>内容来自 <a href=\"https://leetcode-cn.com/problems/generate-parentheses/solution/hui-su-suan-fa-by-liweiwei1419\" target=\"_blank\" rel=\"noopener\">liweiwei1419</a></h4><blockquote>\n<ol>\n<li>回溯算法本质上是树形问题的深度优先遍历，深度优先遍历有回退的过程，就需要状态重置 (就是回溯) ；</li>\n<li>但如果状态变量是字符串的时候，因为字符串的不可变性，在拼接过程中会产生新的字符串，因此每一个状态 (树中的每一个节点) 都是新的字符串，可以不用显式回溯；</li>\n<li>深度优先遍历用递归去实现的原因：<br>(1) 树形结构本身就是递归定义的<br>(2) 深度优先遍历要用到 stack, 但可以用函数的栈，把需要的节点状态变量设置为函数的参数，这样就不用再写一个节点类去完成深度优先遍历。</li>\n</ol>\n</blockquote>\n<hr>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a><img src=\"/images/pasted-0.png\" srcset=\"/img/loading.gif\" alt=\"upload successful\"></h2><pre><code>class Solution:\n    def generateParenthesis(self, n: int) -&gt; List[str]:\n        cut_str &#x3D; &quot;&quot;\n        res &#x3D; []\n        def dfs(cur_str, left, right): # 输入当前字符串内容, 剩余左括号数量, 剩余右括号数量\n        \tif left &#x3D;&#x3D; 0 and right &#x3D;&#x3D; 0:\n            \tres.append(cur_str)\n                return\n            if left &gt; right:\n            \treturn\n            if left &gt; 0:\n            \tdfs(cur_str + &quot;(&quot;, left - 1, right)\n            if right &gt; 0:\n            \tdfs(cur_str + &quot;)&quot;, ledt, right - 1)\n        dfs(cur_str, n, n)\n        return res</code></pre>"}],"PostAsset":[],"PostCategory":[{"post_id":"ckb91ko3o0000y574buwpfr0f","category_id":"ckb91ko3u0001y5743qczhn27","_id":"ckb91ko3y0004y5741k5gffx2"},{"post_id":"ckb91ko4i0005y574a4z111un","category_id":"ckb91ko4j0006y574dn684sxw","_id":"ckb91ko4l0009y57412kt7box"}],"PostTag":[{"post_id":"ckb91ko3o0000y574buwpfr0f","tag_id":"ckb91ko3v0002y5745y9yhcna","_id":"ckb91ko3x0003y5745qlag2sc"},{"post_id":"ckb91ko4i0005y574a4z111un","tag_id":"ckb91ko4k0007y574hash4quw","_id":"ckb91ko4l0008y57445fw8fml"}],"Tag":[{"name":"问题生成","_id":"ckb91ko3v0002y5745y9yhcna"},{"name":"回溯","_id":"ckb91ko4k0007y574hash4quw"}]}}